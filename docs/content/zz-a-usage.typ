#import "../utils/constants.typ": TheTool, git_repo_link
#import "../utils/global-imports.typ": codly
#import codly: codly
#codly(number-format: none)

= Usage Instructions <usage>

The instructions below explain how to build and run #TheTool,
as well as how to obtain a dataset of packages and repositories.

A copy of the instructions below,
along with the necessary source files,
can be found in the Git repository accompanying this report#footnote(git_repo_link).

== Building

Building this project and its associated tool (#TheTool) requires Nix to be installed.
Nix can be downloaded from the NixOS website#footnote(link("https://nixos.org/download")).

Once Nix is installed, the following command can be run in the repository's root directory
to enter a development shell with all required dependencies:

```sh
nix develop -f shell.nix
```

This command is expected to take longer the first time it is run,
given Nix might compile Pysa and other required dependencies.
In subsequent runs, entering the development shell should be significantly faster,
as the build results will be cached.

Once inside the development shell, #TheTool can be compiled using cargo:

```sh
cd tool
cargo build --release
```

This will create a binary at `./target/release/classa`.
If preferred, it can also be run using cargo, which will (re)build the binary when needed:

```sh
cargo run --release
```

== Usage

An overview of #TheTool's functionality can be seen by passing the `--help`
flag to the #TheTool binary:

#[
  #set text(size: 0.75em)
  ```
  $ ./target/release/classa --help
  Find class pollution in Python programs

  Usage: classa [OPTIONS] <COMMAND>

  Commands:
    analyse  Run analysis on a Python program and try to find class pollution
    e2e      Run an end-to-end pipeline, analysing the projects declared in the given dataset
    results  Parse results from a pysa run and summarise them
    label    Parse reports from a previous e2e run, show issues, and ask for appropriate labels
    summary  Parse reports from a previous e2e run, and compile it into a JSON file that be used for charts
    help     Print this message or the help of the given subcommand(s)

  Options:
        --pyre-path <PYRE_PATH>  Path to the pyre (Python) program. If not provided, tries to find it in PATH [env: PYRE_PATH=]
        --workdir <WORKDIR>      A path to the work directory to use, for storing files during the analysis and also final reports, when applicable [env: WORKDIR=]
        --keep-workdir           Whether to keep the work directory after exiting, instead of deleting it. This is implicitly true if the --workdir option is given [env: KEEP_WORKDIR=]
    -h, --help                   Print help
    -V, --version                Print version
  ```
]

Further help about each command is available by running `classa <COMMAND> --help`.

Below are some common commands that could be relevant to run.
For simplicity, it is assumed the binary is named `classa`,
but `./target/release/classa` or `cargo run --release --` might be used instead.

The `WORKDIR` environment variable,
or the `--workdir` flag, should also be set,
otherwise the detailed results will be immediately deleted.

- *Analyse a single Python program that is already present in the file system:*

  ```sh
  classa analyse /path/to/project
  ```

- *Analyse projects in bulk from a dataset* \
  _Note: To generate a dataset, see @usage:dataset. _

  ```sh
  classa e2e /path/to/dataset.toml
  ```

  After the analysis is completed, detailed information about the results of each project
  can be found in their individual reports in the `<workdir>/reports` directory.

- *Label results present in an existing workdir* \
  _Note: Ensure the workdir is the same generated by a e2e run._

  ```sh
  classa label
  ```

- *Generate summary of an e2e analysis* \
  _Note: Ensure the workdir is the same generated by a e2e run._

  ```sh
  classa summary
  ```

  After running this command,
  a `summary.json` file inside the workdir will contain an overview of all analysed projects
  along with the issues found.

== Generating the Dataset <usage:dataset>

The `data/` directory contains the scripts used to obtain
the dataset of repositories and projects the tool has been be tested against.
While it is possible to run the scripts manually to recreate the dataset (instructions below),
a pre-compiled dataset is included in the releases tab of the accompanying repository
for convenience and reproducibility.

Before generating a combined dataset, it is necessary to generate a dataset for
both GitHub repositories and PyPI packages.

=== GitHub

Running `./github/01_get_repos.py` will generate a list of all the Python GitHub repos with
at least 1000 stars.
The script supports resuming a stopped session, and respects GitHub's rate-limit.
This then needs to be pre-processed by another script, since it saves the raw response
from GitHub.

Then, running `./github/02_fetch_refs.py` will fetch the latest commit in the default branch
of all repositories.
This script also supports resuming a stopped session.

Finally, running `./github/03_process_raw_gh_data.py` will generate a JSON file that
merges the previously fetched data and strips out unnecessary information.

=== PyPI

Running `./pypi/get-top-packages.py` will fetch the latest version
of the top 15 000 @pypi packages,
and pick an appropriate wheel or source distribution for each one.

=== Final Dataset

Running `./gen_input_dataset.py` will generate a `dataset.toml` in the current
directory, containing a random subset of packages.
This file can be fed directly into #TheTool,
which will analyse these packages for class pollution vulnerabilities.

#codly(number-format: numbering.with("1"))
